{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb08633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def csvs_to_sqlite(csv_dir, sqlite_path=\"output.db\"):\n",
    "    # Connect (or create) SQLite database\n",
    "    conn = sqlite3.connect(sqlite_path)\n",
    "\n",
    "    # Loop over all CSV files in directory\n",
    "    for file in os.listdir(csv_dir):\n",
    "        if file.endswith(\".csv\"):\n",
    "            # Extract table name from filename: keep middle part\n",
    "            # e.g. beataml2__patient__20250924071940.csv -> patient\n",
    "            parts = file.split(\"__\")\n",
    "            if len(parts) >= 3:\n",
    "                table_name = parts[1]\n",
    "            else:\n",
    "                table_name = os.path.splitext(file)[0]  # fallback to filename\n",
    "\n",
    "            print(f\"Loading file: {file} into table: {table_name}\")\n",
    "            \n",
    "            # Read CSV\n",
    "            df = pd.read_csv(os.path.join(csv_dir, file))\n",
    "            \n",
    "            # Write DataFrame to SQLite\n",
    "            df.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"All CSV files from {csv_dir} are now stored in {sqlite_path}\")\n",
    "\n",
    "\n",
    "csvs_to_sqlite(\"BeatAML/Tables\", sqlite_path=\"BeatAML/BeatAML.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2026fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_llm_description(prompt, model=\"gpt-4o-mini\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert data documentation assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def is_probably_numeric_string(series, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Detect if a string column is mostly numeric.\n",
    "    If >= threshold fraction can be converted to numeric, treat as numeric.\n",
    "    \"\"\"\n",
    "    non_null = series.dropna().astype(str)\n",
    "    converted = pd.to_numeric(non_null, errors=\"coerce\")\n",
    "    numeric_ratio = converted.notna().mean()\n",
    "    return numeric_ratio >= threshold\n",
    "\n",
    "\n",
    "def generate_schema_from_db(\n",
    "    db_path, \n",
    "    field_dict_csv=None, \n",
    "    max_unique=50, \n",
    "    sample_size=5, \n",
    "    max_workers=10\n",
    "):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    schema = {}\n",
    "    concept_rows = []\n",
    "    \n",
    "    # --- NEW: integrate field dictionary ---\n",
    "    field_dict = {}\n",
    "    if field_dict_csv:\n",
    "        fd_df = pd.read_csv(field_dict_csv).dropna(subset=[\"Table Name\", \"fields \"])\n",
    "        for _, row in fd_df.iterrows():\n",
    "            t = str(row[\"Table Name\"]).strip()\n",
    "            f = str(row[\"fields \"]).strip()\n",
    "            field_dict[(t, f)] = {\n",
    "                \"dtype\": str(row.get(\"data type\", \"\")).strip() if not pd.isna(row.get(\"data type\", \"\")) else None,\n",
    "                \"description\": str(row.get(\"description \", \"\")).strip() if not pd.isna(row.get(\"description \", \"\")) else None\n",
    "            }\n",
    "    \n",
    "    # Get table names\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [t[0] for t in cursor.fetchall()]\n",
    "    \n",
    "    for table in tables:\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table}\", conn)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Build richer table-level prompt\n",
    "        # ----------------------------\n",
    "        field_examples = []\n",
    "        for col in df.columns:\n",
    "            sample_vals = pd.Series(df[col].dropna().unique()).head(sample_size).tolist()\n",
    "            dtype = str(df[col].dtype)\n",
    "            field_examples.append(f\"- {col} ({dtype}), e.g. {sample_vals}\")\n",
    "\n",
    "        table_prompt = f\"\"\"\n",
    "        You are given the following information about a database table:\n",
    "\n",
    "        Table name: {table}\n",
    "\n",
    "        Columns with example values:\n",
    "        {chr(10).join(field_examples)}\n",
    "\n",
    "        Please write a clear, concise (2–3 lines), human-readable and accurate description of what this table contains overall. Focus on the purpose and nature of data stored in the table, not on detailed descriptions of individual fields. Provide your response in a plain text format.\n",
    "        \"\"\"\n",
    "\n",
    "        # ----------------------------\n",
    "        # Collect prompts (table + fields)\n",
    "        # ----------------------------\n",
    "        prompts = {\"__table__\": table_prompt}\n",
    "        candidate_concepts = []  # store candidate concept values only after filtering\n",
    "\n",
    "        for col in df.columns:\n",
    "            col_data = df[col].dropna()\n",
    "            dtype = str(df[col].dtype)\n",
    "\n",
    "            total_rows = len(df)\n",
    "            num_unique = col_data.nunique()\n",
    "            uniqueness_pct = (num_unique / total_rows) * 100 if total_rows > 0 else 0\n",
    "            \n",
    "            unique_vals = col_data.unique()\n",
    "            if len(unique_vals) > max_unique:\n",
    "                unique_info = f\"{len(unique_vals)} unique values\"\n",
    "                concept_samples = unique_vals\n",
    "            else:\n",
    "                unique_info = unique_vals.tolist()\n",
    "                concept_samples = unique_vals\n",
    "\n",
    "            # ---- Exclusion logic for concept table ----\n",
    "            exclude = False\n",
    "            if np.issubdtype(df[col].dtype, np.number):\n",
    "                exclude = True  # numeric column\n",
    "            elif is_probably_numeric_string(col_data):\n",
    "                exclude = True  # stringified numeric\n",
    "            elif uniqueness_pct == 100:\n",
    "                exclude = True  # ID-like column\n",
    "            \n",
    "            if not exclude:\n",
    "                candidate_concepts.append((table, col, concept_samples))\n",
    "\n",
    "            # ---- Field description prompt ----\n",
    "            fd_info = field_dict.get((table, col), {})\n",
    "            dict_dtype = fd_info.get(\"dtype\")\n",
    "            dict_desc = fd_info.get(\"description\")\n",
    "\n",
    "            extra_context = \"\"\n",
    "            if dict_dtype or dict_desc:\n",
    "                extra_context = f\"\"\"\n",
    "                Additional context from data dictionary:\n",
    "                - Declared data type: {dict_dtype}\n",
    "                - Provided description: {dict_desc}\n",
    "                \"\"\"\n",
    "\n",
    "            field_prompt = f\"\"\"\n",
    "            You are given the following information about a database column:\n",
    "\n",
    "            - Column name: {col}\n",
    "            - Table: {table}\n",
    "            - Data type (inferred from DB): {dtype}\n",
    "            - Sample values: {pd.Series(col_data.unique()).head(sample_size).tolist()}\n",
    "            - Unique values: {unique_info}\n",
    "            - Uniqueness percent: {round(uniqueness_pct, 2)}\n",
    "            {extra_context}\n",
    "\n",
    "            Write a clear, concise and human-readable description of what this field likely represents. Provide your response as a plain text format.\n",
    "            \"\"\"\n",
    "            prompts[col] = field_prompt\n",
    "\n",
    "        # ----------------------------\n",
    "        # Parallel execution of all prompts\n",
    "        # ----------------------------\n",
    "        results = {}\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_key = {\n",
    "                executor.submit(generate_llm_description, p): k\n",
    "                for k, p in prompts.items()\n",
    "            }\n",
    "            for future in concurrent.futures.as_completed(future_to_key):\n",
    "                key = future_to_key[future]\n",
    "                try:\n",
    "                    results[key] = future.result()\n",
    "                except Exception as e:\n",
    "                    results[key] = f\"Error: {e}\"\n",
    "\n",
    "        # ----------------------------\n",
    "        # Build schema with results\n",
    "        # ----------------------------\n",
    "        table_info = {\n",
    "            \"table_description\": results[\"__table__\"],\n",
    "            \"fields\": {}\n",
    "        }\n",
    "\n",
    "        for col in df.columns:\n",
    "            col_data = df[col].dropna()\n",
    "            dtype = str(df[col].dtype)\n",
    "\n",
    "            total_rows = len(df)\n",
    "            num_unique = col_data.nunique()\n",
    "            uniqueness_pct = (num_unique / total_rows) * 100 if total_rows > 0 else 0\n",
    "            \n",
    "            unique_vals = col_data.unique()\n",
    "            if len(unique_vals) > max_unique:\n",
    "                unique_info = f\"{len(unique_vals)} unique values\"\n",
    "            else:\n",
    "                unique_info = unique_vals.tolist()\n",
    "\n",
    "            table_info[\"fields\"][col] = {\n",
    "                \"field_data_type\": dtype,\n",
    "                \"field_description\": results[col],\n",
    "                \"field_sample_values\": pd.Series(col_data.unique()).head(sample_size).tolist(),\n",
    "                \"field_unique_values\": unique_info,\n",
    "                \"field_uniqueness_percent\": round(uniqueness_pct, 2)\n",
    "            }\n",
    "\n",
    "        schema[table] = table_info\n",
    "\n",
    "        # ----------------------------\n",
    "        # Finalize concept table (after LLM review)\n",
    "        # ----------------------------\n",
    "        for table_name, col_name, values in candidate_concepts:\n",
    "            for val in values:\n",
    "                concept_rows.append({\n",
    "                    \"concept_name\": str(val),\n",
    "                    \"table_name\": table_name,\n",
    "                    \"field_name\": col_name\n",
    "                })\n",
    "\n",
    "    concept_df = pd.DataFrame(concept_rows)\n",
    "    return schema, concept_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "db_path = \"BeatAML/BeatAML.db\"  # path to SQLite DB\n",
    "field_dict_csv = \"BeatAML/BeatAML_data_dict.csv\"  # path to your CSV with descriptions\n",
    "schema, concept_df = generate_schema_from_db(db_path, field_dict_csv=field_dict_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f90f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Assuming schema is already generated from your function\n",
    "output_file = \"BeatAML/BeatAML_schema.json\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(schema, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Schema saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f33ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df[\"concept_with_context\"] = concept_df[\"table_name\"].astype(str) + \"_\" + concept_df[\"field_name\"].astype(str) + \"_\" + concept_df[\"concept_name\"].astype(str)\n",
    "concept_df.to_csv(\"BeatAML/concept_table.csv\", index=False)\n",
    "concept_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926afb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# -------- Step 1: Load concept table --------\n",
    "concept_df = pd.read_csv(\"BeatAML/concept_table.csv\")\n",
    "concepts = concept_df[\"concept_with_context\"].astype(str).tolist()\n",
    "\n",
    "# -------- Step 2: Define embedding function --------\n",
    "def get_single_embedding(text, model=\"text-embedding-3-small\", max_retries=3, delay=2):\n",
    "    \"\"\"Generate embedding for a single text with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.embeddings.create(model=model, input=text)\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(delay * (2 ** attempt))  # exponential backoff\n",
    "            else:\n",
    "                print(f\"[ERROR] Failed after {max_retries} retries for text: {text[:50]}... | Error: {e}\")\n",
    "                return None\n",
    "\n",
    "# -------- Step 3: Parallel embedding generation --------\n",
    "def get_embeddings_parallel(texts, model=\"text-embedding-3-small\", max_workers=10):\n",
    "    embeddings = [None] * len(texts)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(get_single_embedding, text, model): i for i, text in enumerate(texts)}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Generating embeddings\"):\n",
    "            idx = futures[future]\n",
    "            result = future.result()\n",
    "            embeddings[idx] = result\n",
    "    return np.array([e if e is not None else np.zeros(1536) for e in embeddings])  # handle failed embeddings\n",
    "\n",
    "# -------- Step 4: Generate and save embeddings --------\n",
    "print(f\"Generating embeddings for {len(concepts)} concepts using parallel threads...\")\n",
    "embeddings = get_embeddings_parallel(concepts, max_workers=10)\n",
    "\n",
    "output_path = \"BeatAML/concept_embeddings.pkl\"\n",
    "with open(output_path, \"wb\") as f:\n",
    "    pickle.dump({\"concepts\": concepts, \"embeddings\": embeddings}, f)\n",
    "\n",
    "print(f\"✅ Embeddings saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca9330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Literal, Tuple\n",
    "from rich import print\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# open the schema json file\n",
    "with open('./BeatAML/BeatAML_schema.json', 'r') as f:\n",
    "    DB_SCHEMA = json.load(f)\n",
    "\n",
    "# helper functions to extract summary text about DB fields and embed them as vectors\n",
    "def extract_table_field_texts(schema: Dict[str, Dict[str, Any]]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extract (table.field, text) pairs from DB schema.\n",
    "    Text includes table/field description + sample values (if present).\n",
    "    Return a dict mapping table.field name to its description text.\n",
    "    \"\"\"\n",
    "    pairs = {}\n",
    "    for table, table_info in schema.items():\n",
    "        table_desc = table_info.get('table_description', {})\n",
    "        fields = table_info.get(\"fields\", {})\n",
    "        for field, field_info in fields.items():\n",
    "            desc = field_info.get(\"field_description\", \"\")\n",
    "            samples = field_info.get(\"field_sample_values\", [])\n",
    "            sample_str = f\" Example Values: {', '.join(map(str, samples))}\" if samples else \"\"\n",
    "            #text = f\"{table}__{table_desc.split('.')[0]}__{field}__{desc}__{sample_str}\" # include the 1st line in table summary \n",
    "            text = f\"{field}__{desc}__{sample_str}\" # only field summary, exclude table summary \n",
    "            pairs[f'{table}.{field}'] = text.strip()\n",
    "    return pairs\n",
    "\n",
    "def embed_texts(pairs: Dict[str, str]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Embed schema table + field texts into vectors.\n",
    "    Returns dict in format: {table.field: {'text': str, 'embedding': List}}\n",
    "    \"\"\"\n",
    "    texts = [text for id, text in pairs.items()]\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\", \n",
    "        input=texts\n",
    "    )\n",
    "    embeddings = [e.embedding for e in response.data]\n",
    "    return {k: {\"text\": v, \"embedding\": embeddings[i]} for i, (k, v) in enumerate(pairs.items())}\n",
    "\n",
    "\n",
    "table_field_pairs = extract_table_field_texts(DB_SCHEMA)\n",
    "\n",
    "# Get table.field embeddings along with texts\n",
    "table_field_embeddings = embed_texts({k:v for k,v in list(table_field_pairs.items())})\n",
    "\n",
    "# Write embeddings dict to file\n",
    "with open('BeatAML/db_table_field_embeddings.json', 'w') as f:\n",
    "    json.dump(table_field_embeddings, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
